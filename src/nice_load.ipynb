{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 15:44:33.272957: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-14 15:44:33.273030: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-14 15:44:33.273036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import datetime\n",
    "import functools\n",
    "import os\n",
    "from typing import Sequence\n",
    "\n",
    "from absl import app, flags\n",
    "from absl import logging\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from ml_collections import config_flags\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from nice import NICE\n",
    "import os\n",
    "from utils import flatten_nested_dict, update_config_dict, setup_training\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def load_dataset(\n",
    "    split: str,\n",
    "    batch_size: int,\n",
    "    im_size: int,\n",
    "    alpha: float,\n",
    "    n_bits: int\n",
    "):\n",
    "  \"\"\"Loads the dataset as a generator of batches.\"\"\"\n",
    "  ds, ds_info = tfds.load(\"mnist\", split=split,\n",
    "                          as_supervised=True, with_info=True)\n",
    "  ds = ds.cache()\n",
    "  ds = ds.map(lambda x, y: resize(x, y, im_size=im_size),\n",
    "              num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  ds = ds.map(lambda x, y: dequantize(x, y, n_bits=n_bits),\n",
    "              num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  ds = ds.map(lambda x, y: logit(x, y, alpha=alpha),\n",
    "              num_parallel_calls=tf.data.AUTOTUNE)\n",
    "  ds = ds.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "  ds = ds.batch(batch_size, drop_remainder=True)\n",
    "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "  return tfds.as_numpy(ds)\n",
    "\n",
    "def dequantize(x, y, n_bits=3):\n",
    "  n_bins = 2. ** n_bits\n",
    "  x = tf.cast(x, tf.float32)\n",
    "  x = tf.floor(x / 2 ** (8 - n_bits))\n",
    "  x = x / n_bins\n",
    "  x = x + tf.random.uniform(x.shape) / n_bins\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def resize(x, y, im_size=28):\n",
    "  \"\"\"Resize images to desired size.\"\"\"\n",
    "  x = tf.image.resize(x, (im_size, im_size))\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def logit(x, y, alpha=1e-6):\n",
    "  \"\"\"Scales inputs to rance [alpha, 1-alpha] then applies logit transform.\"\"\"\n",
    "  x = x * (1 - 2 * alpha) + alpha\n",
    "  x = tf.math.log(x) - tf.math.log(1. - x)\n",
    "  return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nice_config import get_config\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "def forward_fn():\n",
    "    flow = NICE(config.im_size ** 2, h_dim=config.hidden_dim)\n",
    "\n",
    "    def _logpx(x):\n",
    "        return flow.logpx(x)\n",
    "    def _recons(x):\n",
    "        return flow.reverse(flow.forward(x))\n",
    "    def _sample():\n",
    "        return flow.sample(config.batch_size)\n",
    "    return _logpx, (_logpx, _recons, _sample)\n",
    "\n",
    "forward = hk.multi_transform(forward_fn)\n",
    "rng_seq = hk.PRNGSequence(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0.05_3_14\n"
     ]
    }
   ],
   "source": [
    "artifact_name = f\"{config.alpha}_{config.n_bits}_{config.im_size}\"\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "artifact = api.artifact(f\"shreyaspadhy/cais/{artifact_name}:latest\")\n",
    "params = pickle.load(open(artifact.file(), \"rb\"))\n",
    "artifact_loaded = True\n",
    "print(f'Loaded {artifact_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.747478692380518\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (1929556741.py, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [25], line 31\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "ds = load_dataset(\"train\", config.batch_size, config.im_size, config.alpha,\n",
    "                    config.n_bits)\n",
    "ds_test = load_dataset(\"test\", config.batch_size, config.im_size,\n",
    "                        config.alpha, config.n_bits)\n",
    "\n",
    "# get init data\n",
    "x, _ = next(iter(ds))\n",
    "x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "logpx_fn, recons_fn, sample_fn = forward.apply\n",
    "\n",
    "# params = forward.init(next(rng_seq), x)\n",
    "x_re = recons_fn(params, next(rng_seq), x)\n",
    "\n",
    "x_sample = sample_fn(params, next(rng_seq))\n",
    "\n",
    "x_sample_2 = sample_fn(params, next(rng_seq))\n",
    "\n",
    "n_samples, dim = x_sample.shape\n",
    "\n",
    "x_sample, x_sample_2 = np.array(x_sample), np.array(x_sample_2)\n",
    "\n",
    "a, b = np.ones((n_samples,)) / n_samples, np.ones((n_samples,)) / n_samples\n",
    "import ot\n",
    "\n",
    "import utils\n",
    "\n",
    "print(utils.W2_distance(x_sample, x_sample_2))\n",
    "\n",
    "break\n",
    "\n",
    "# log_prob = logpx_fn(params, next(rng_seq), x)\n",
    "\n",
    "log_prob_fn = lambda x: logpx_fn(params, next(rng_seq), x)\n",
    "\n",
    "log_prob = log_prob_fn(x)\n",
    "print(log_prob.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print(x.shape, x_re.shape, x_sample.shape)\n",
    "\n",
    "# plt.imshow(x[0].reshape(28, 28))\n",
    "# plt.show()\n",
    "# plt.imshow(x_re[0].reshape(28, 28))\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldvi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
